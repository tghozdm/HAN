# han/hafiza.py (GÃœNCELLENMÄ°Åž HALÄ°)
import torch
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from typing import List, Optional

class HafizaMotoru:
    def __init__(
        self, 
        model_name: str = "Qwen/Qwen2.5-1.5B-Instruct", # Ä°stersen 7B yap
        embedder_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        device: Optional[str] = None,
        load_in_4bit: bool = True  # <--- YENÄ° Ã–ZELLÄ°K: VarsayÄ±lan 4-Bit
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ðŸš€ HAN baÅŸlatÄ±lÄ±yor (Cihaz: {self.device.upper()})")
        
        # 4-BIT AYARLARI (RAM Tasarrufu)
        quantization_config = None
        if load_in_4bit and self.device == "cuda":
            print("ðŸ’¡ 4-Bit SÄ±kÄ±ÅŸtÄ±rma Aktif (DÃ¼ÅŸÃ¼k VRAM Modu)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True
            )

        print(f"ðŸ“¥ LLM yÃ¼kleniyor: {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config, # <--- EKLENDÄ°
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None
        )
        
        print("ðŸ“¥ Embedding modeli yÃ¼kleniyor...")
        self.embedder = SentenceTransformer(embedder_name)
        self.dimension = self.embedder.get_sentence_embedding_dimension()
        
        self.index = None
        self.stored_docs = []
        print("âœ… HAN hazÄ±r!")

    # ... (DiÄŸer fonksiyonlar: verileri_yukle, ara, soru_sor AYNI KALACAK) ...
    # Sadece yukarÄ±daki __init__ kÄ±smÄ±nÄ± deÄŸiÅŸtirmen yeterli.
    
    # KODUN DEVAMINI AYNEN KORU (verileri_yukle, ara, soru_sor, kaydet, yukle)
    def verileri_yukle(self, metin_listesi: List[str], batch_size: int = 32):
        # ... (AynÄ± kod) ...
        if not metin_listesi:
            raise ValueError("Metin listesi boÅŸ olamaz")
        
        print(f"ðŸ”„ {len(metin_listesi)} belge iÅŸleniyor...")
        self.stored_docs = metin_listesi
        
        embeddings = self.embedder.encode(
            metin_listesi,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(embeddings.astype('float32'))
        
        print(f"âœ… {len(metin_listesi)} belge hafÄ±zaya eklendi")

    def ara(self, soru: str, k: int = 3, min_score: float = 2.0) -> List[str]:
        # ... (AynÄ± kod) ...
        if self.index is None or len(self.stored_docs) == 0:
            return []
        
        k = min(k, len(self.stored_docs))
        soru_vektoru = self.embedder.encode([soru], convert_to_numpy=True).astype('float32')
        mesafeler, indeksler = self.index.search(soru_vektoru, k)
        
        bulunanlar = []
        for mesafe, idx in zip(mesafeler[0], indeksler[0]):
            if idx < len(self.stored_docs): # min_score kontrolÃ¼nÃ¼ opsiyonel yapabilirsin
                bulunanlar.append(self.stored_docs[idx])
        
        return bulunanlar

    def soru_sor(self, soru: str, k: int = 3, max_tokens: int = 150, temperature: float = 0.1) -> str:
        # ... (AynÄ± kod) ...
        baglam_listesi = self.ara(soru, k=k)
        
        if not baglam_listesi:
            baglam = "Ä°lgili bilgi bulunamadÄ±."
        else:
            baglam = "\n---\n".join(baglam_listesi)
        
        messages = [
            {"role": "system", "content": "Sen yardÄ±mcÄ± bir asistansÄ±n. Sadece verilen BAÄžLAM bilgisini kullanarak cevap ver. Bilgi yoksa 'Bilmiyorum' de."},
            {"role": "user", "content": f"BAÄžLAM:\n{baglam}\n\nSORU: {soru}"}
        ]
        
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.llm.generate(
                inputs.input_ids,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        return response.strip()

    def kaydet(self, dosya_adi: str):
        faiss.write_index(self.index, f"{dosya_adi}.index")
        np.save(f"{dosya_adi}_docs.npy", self.stored_docs)
        print(f"ðŸ’¾ HafÄ±za kaydedildi: {dosya_adi}")
    
    def yukle(self, dosya_adi: str):
        self.index = faiss.read_index(f"{dosya_adi}.index")
        self.stored_docs = np.load(f"{dosya_adi}_docs.npy", allow_pickle=True).tolist()
        print(f"ðŸ“‚ HafÄ±za yÃ¼klendi: {dosya_adi}")
